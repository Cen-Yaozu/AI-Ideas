# 马尔可夫思维：从计算复杂度到上下文工程的范式转变

## 摘要

本文从理论与实践两个维度探讨马尔可夫链在大语言模型推理中的应用，特别是 McGill NLP 实验室提出的 "The Markovian Thinker" 方法。我们认为，这不仅是一个工程优化问题，更是对 AI 上下文工程（Context Engineering）的根本性重新思考——从"记住一切"到"携带必要信息"的认知范式转变。

**关键词**：马尔可夫链、上下文工程、计算复杂度、强化学习、元学习

---

## 一、马尔可夫链：被遗忘的数学基础

### 1.1 什么是马尔可夫链

马尔可夫链（Markov Chain）是一种描述系统状态转移的随机过程模型，其核心特性是**马尔可夫性**（Markov Property），也称为**无记忆性**：

$$P(X_{n+1} = s \mid X_n, X_{n-1}, \ldots, X_0) = P(X_{n+1} = s \mid X_n)$$

用人类语言表达：**系统的下一个状态只依赖于当前状态，与过去的历史无关。**

### 1.2 基础组成

一个马尔可夫链由三个核心要素构成：

1. **状态空间** $S = \{s_1, s_2, \ldots, s_n\}$：系统可能处于的所有状态集合
2. **转移概率** $P_{ij} = P(X_{t+1} = s_j \mid X_t = s_i)$：从状态 $s_i$ 转移到状态 $s_j$ 的概率
3. **转移矩阵** $P$：所有转移概率组成的矩阵

**经典例子：天气模型**

```
状态空间：{晴天, 雨天}
转移矩阵：
         晴    雨
    晴 [0.7  0.3]
    雨 [0.4  0.6]
```

如果今天晴天，明天有 70% 概率仍是晴天，30% 概率转为雨天。

### 1.3 重要性质

1. **平稳分布**（Stationary Distribution）：经过足够多步转移后，状态分布趋于稳定
2. **遍历性**（Ergodicity）：从任意状态可以到达任意其他状态
3. **不可约性**（Irreducibility）：所有状态形成一个连通整体

---

## 二、大语言模型中的马尔可夫假设

### 2.1 LLM 本质上是马尔可夫过程

现代大语言模型的文本生成过程可以建模为马尔可夫链：

$$P(\text{token}_{t+1} \mid \text{token}_1, \ldots, \text{token}_t) \approx P(\text{token}_{t+1} \mid \text{context window}_t)$$

关键观察：
- **理论上**：模型应该考虑所有历史 token
- **实际上**：只能处理固定长度的上下文窗口（如 8K、128K）
- **本质上**：这就是一个近似的马尔可夫过程

### 2.2 传统方法的困境

在推理任务（如数学证明、代码生成）中，传统的 RLVR（Reinforcement Learning for Reasoning）方法定义状态为：

$$\text{State}_t = [\text{原始问题}, \text{token}_1, \text{token}_2, \ldots, \text{token}_t]$$

这导致：
- **状态空间无限增长**：随着思考深入，状态越来越长
- **计算复杂度二次方**：自注意力机制的复杂度 $O(n^2)$
- **内存占用线性增长**：KV cache 不断累积

**这是一个被遗忘的事实**：大家在优化模型架构，却忽略了底层 MDP（Markov Decision Process）设计的不合理性。

---

## 三、The Markovian Thinker：重新设计 MDP

### 3.1 核心洞察

McGill NLP 实验室的工作提出一个根本性问题：

> **为什么状态必须包含所有历史？能否设计一个状态大小有界的 MDP？**

答案是 **Delethink** 方法：通过强制状态大小固定，让模型学会在有限信息中携带推理进度。

### 3.2 Delethink 工作原理

#### 3.2.1 传统方法（LongCoT）

```
第0轮：生成 token[1] 到 token[8192]
状态 = [问题, token[1], ..., token[8192]]

第1轮：基于完整历史生成 token[8193] 到 token[16384]
状态 = [问题, token[1], ..., token[16384]]

→ 状态长度：8K → 16K → 24K → ...
→ 计算成本：O(8K²) → O(16K²) → O(24K²) → ...
```

#### 3.2.2 Delethink 方法

```python
# 关键参数
C = 8192    # 上下文窗口大小（context_size）
m = 4096    # 马尔可夫状态大小（markovian_size）
I = 5       # 最大迭代次数

# 第0轮
生成最多 C 个 token
保留前 100 个 token 到 query_ids
状态 = [问题, token[1], ..., token[100], ..., token[C]]

# 第1轮及之后
每轮：
  1. 从状态开始生成最多 (C - m) 个新 token
  2. 重置上下文：状态 = [问题, query_ids, last_m_tokens]
  3. 清空 KV cache（强制重新编码）

→ 状态长度：始终 ≤ 8K
→ 计算成本：O(8K²) × I （线性于迭代次数）
```

**总推理预算**：
$$\text{Total Tokens} = C + (I-1) \times (C-m) = 8192 + 4 \times 4096 = 24576$$

### 3.3 关键代码解析

从 `delethink_tracing_demo.py` 的核心循环：

```python
while iterations < max_iterations:
    # 第一轮：完整预算；后续轮：只用非马尔可夫部分
    params["max_new_tokens"] = (
        context_size - markovian_size if iterations > 0
        else context_size
    )

    # 生成下一块
    response = await llm.async_generate(
        input_ids=prompt_ids,
        sampling_params=params
    )
    response_ids = _get_output_ids(response)

    # 如果达到 EOS，停止
    if response["meta_info"]["finish_reason"]["type"] == "stop":
        break

    # 关键：下一轮的 prompt = query_ids + 最后 m 个 token
    prompt_ids = query_ids + response_ids[-markovian_size:]

    iterations += 1
```

**这段代码的深刻之处**：
1. 每次 `prompt_ids` 重置，KV cache 被清空
2. 只保留最后 `m` 个 token 作为"记忆携带"
3. 模型被迫学会在有限状态中编码推理进度

---

## 四、理论贡献：从工程到认知科学

### 4.1 重新定义状态空间

**传统范式**：
$$\text{State} = \text{History}$$

**马尔可夫范式**：
$$\text{State} = \text{Sufficient Statistics}(\text{History})$$

Delethink 实际上在训练模型学习一个**充分统计量提取函数**：

$$f: \text{History} \to \text{Fixed-Size State}$$

这使得：
$$P(\text{next} \mid \text{full history}) \approx P(\text{next} \mid f(\text{last m tokens}))$$

### 4.2 元学习视角

Delethink 不仅是优化技巧，更是一种**元学习**（Meta-Learning）过程：

- **学习内容**：如何在有限信息中携带推理进度
- **学习机制**：通过 RL 强化那些能在马尔可夫约束下解决问题的策略
- **涌现能力**：模型学会了"压缩关键信息"而非"记住所有细节"

这类似于人类认知：
- ❌ 不是逐字记忆推理过程
- ✅ 而是提取关键结论、中间结果、待解决子问题

### 4.3 与认知科学的联系

**工作记忆**（Working Memory）理论：
- 人类工作记忆容量有限（约 7±2 个单位）
- 但可以通过"分块"（Chunking）扩展有效容量
- Delethink 的 `m` 参数就像工作记忆容量限制

**外部记忆**（External Memory）：
- query_ids 保留的前 100 个 token 像"笔记本"
- 每轮重新加载，类似查阅笔记

---

## 五、实验结果与分析

### 5.1 计算效率

| 方法 | 上下文长度 | 计算复杂度 | 训练成本（H100小时） |
|------|-----------|-----------|---------------------|
| LongCoT-8K | 8K | $O(8K^2)$ | ~100 |
| LongCoT-24K | 24K | $O(24K^2)$ | ~900 |
| Delethink-24K | 8K | $O(8K^2) \times 3$ | ~300 |

**关键发现**：
- Delethink 使用 8K 上下文达到 24K 的推理预算
- 训练成本降低约 **3倍**
- 计算曲线从二次方变为线性

### 5.2 准确率对比

在 DeepScaleR 数学问题数据集上：

```
预算内（≤24K tokens）：
  LongCoT-24K:  65.3%
  Delethink-24K: 66.1% ✓ 略优

超出预算（>24K tokens）：
  LongCoT-24K:  65.8% （趋于平台期）
  Delethink-24K: 70.2% ✓ 持续改进
```

**洞察**：马尔可夫约束不仅不损害性能，反而提升了泛化能力。

### 5.3 扩展性验证

Delethink 已成功扩展到：
- **96K tokens**：5倍于训练预算
- **128K tokens**：理论上可以继续扩展

而 LongCoT 在 24K 后基本停滞。

### 5.4 现有模型的马尔可夫能力

论文发现 GPT-o1 和 Qwen3-30B 在**零样本**情况下就展现出马尔可夫思考能力：

```
实验：强制删除历史，只保留最后 4K tokens
结果：准确率仅下降 5-10%

结论：SOTA 模型已经隐式学会了马尔可夫思维！
```

---

## 六、对上下文工程的启示

### 6.1 上下文工程的本质

传统观念：
> "给 AI 提供越多上下文越好"

新认识：
> "关键在于提供**有效的、结构化的**上下文"

### 6.2 设计原则

#### 原则1：有界状态优于无界历史

**反模式**：
```
提示词：[完整聊天历史 + 完整文档 + 新问题]
```

**良好实践**：
```
提示词：[
  任务描述（固定）
  + 上一轮的关键结论（摘要）
  + 当前子问题（聚焦）
]
```

#### 原则2：压缩优于保留

类似 Delethink 的 query_ids + last_m_tokens 设计：

```
固定锚点（问题、规则）
+
动态状态（最近进展、待解决项）
```

#### 原则3：显式记忆管理

借鉴 RAG（Retrieval-Augmented Generation）思想：
- 将长历史存储到外部向量数据库
- 每轮检索相关的 k 个片段
- 维持固定大小的"工作上下文"

### 6.3 实践建议

#### 场景1：长对话管理

```python
# 传统方法（状态爆炸）
context = [msg1, msg2, ..., msg_100, new_question]

# 马尔可夫方法（有界状态）
context = [
    system_prompt,          # 固定
    conversation_summary,   # 压缩历史
    last_3_messages,        # 马尔可夫状态
    new_question           # 当前输入
]
```

#### 场景2：代码生成

```python
# 低效
prompt = f"{entire_codebase}\n\n请修改函数 foo()"

# 高效
prompt = f"""
项目结构：{structure_summary}
相关依赖：{imports_of_foo}
函数定义：{foo_signature}
最近修改：{recent_changes_summary}

任务：请修改函数 foo() 以支持...
"""
```

#### 场景3：多轮推理

借鉴 Delethink 的"分块思考"：

```python
for iteration in range(max_iterations):
    # 每轮重置上下文
    prompt = build_prompt(
        original_problem=problem,
        progress_summary=summary,  # 压缩的进度
        current_focus=focus        # 当前子问题
    )

    response = llm.generate(prompt, max_tokens=chunk_size)

    # 提取关键信息用于下一轮
    summary = extract_key_points(response)
    focus = identify_next_step(response)
```

---

## 七、哲学与批判性思考

### 7.1 "记忆"的本质

**问题**：AI 需要"记住"所有细节吗？

Delethink 的答案：**不需要**。关键是携带**推理进度的充分表示**。

这挑战了我们对"智能"的直觉：
- 智能不是记忆的堆砌
- 而是信息的有效组织和检索

### 7.2 效率与能力的辩证

传统观点认为：
> "限制上下文会损害模型能力"

但实验表明：
> "适当的限制反而促进更好的泛化"

这类似于正则化（Regularization）的哲学：
- 约束迫使模型学习更本质的模式
- 而非记忆训练数据的表面特征

### 7.3 工程与认知的融合

Delethink 打破了"纯工程优化"与"认知建模"的界限：

```
工程动机：降低计算成本
    ↓
设计约束：固定状态大小
    ↓
涌现能力：学会压缩关键信息
    ↓
认知对应：类人类工作记忆机制
```

这是一个**由工程需求驱动的认知科学发现**。

---

## 八、未来方向与开放问题

### 8.1 理论问题

1. **最优马尔可夫状态大小**：
   - 如何为不同任务确定 `m` 的值？
   - 是否存在理论下界？

2. **充分统计量的学习**：
   - 能否显式建模状态压缩函数 $f$？
   - 能否可视化"模型在携带什么信息"？

3. **多粒度马尔可夫性**：
   - 能否设计分层状态（粗粒度摘要 + 细粒度细节）？

### 8.2 技术扩展

1. **自适应状态大小**：
   - 简单问题用小 `m`，复杂问题用大 `m`
   - 动态调整 `m` 而非固定

2. **结构化状态表示**：
   - 当前：纯文本的最后 `m` 个 token
   - 改进：结构化的键值对（如 JSON）

3. **与 RAG 结合**：
   - 外部向量数据库存储长历史
   - 马尔可夫状态只包含"指针"和"摘要"

### 8.3 应用探索

1. **超长文档理解**：
   - 分块处理 + 马尔可夫状态传递

2. **持续学习**：
   - 无限对话中的记忆管理

3. **多智能体协作**：
   - 智能体间传递马尔可夫状态而非完整历史

---

## 九、结论

### 9.1 核心贡献

The Markovian Thinker 的价值不仅在于技术创新，更在于**范式转变**：

1. **技术层面**：从二次方复杂度降到线性
2. **理论层面**：重新审视 MDP 设计的自由度
3. **认知层面**：揭示"有界理性"可能优于"无限记忆"
4. **工程层面**：为上下文工程提供理论支撑

### 9.2 对上下文工程的启示

**传统思维**：
```
更多上下文 → 更多信息 → 更好性能
```

**马尔可夫思维**：
```
合适的状态表示 → 充分的信息 → 更好泛化
            ↓
        更低成本
```

### 9.3 最终思考

马尔可夫链这个诞生于 20 世纪初的数学工具，在 AI 时代焕发新生。它提醒我们：

> **有时候，限制不是障碍，而是通往更优解的路径。**

在追求更大模型、更长上下文的军备竞赛中，Delethink 提供了一个反思的契机：

**也许，智能的本质不是记住一切，而是知道什么值得记住。**

---

## 参考文献

1. Aghajohari, M., Chitsaz, K., Kazemnejad, A., et al. (2025). "The Markovian Thinker". *arXiv preprint arXiv:2510.06557*.
2. Markov, A. A. (1906). "Rasprostranenie zakona bol'shih chisel na velichiny, zavisyaschie drug ot druga". *Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete*, 2(15), 135-156.
3. Vaswani, A., et al. (2017). "Attention is All You Need". *NeurIPS*.
4. OpenAI (2024). "GPT-4 Technical Report".

---

## 附录：关键代码片段

### A.1 Delethink 核心循环

```python
async def delethink_tracing(
    llm: sgl.Engine,
    sample: dict,
    context_size: int,      # C = 8192
    markovian_size: int,    # m = 4096
    max_iterations: int,    # I = 5
):
    tokenizer = llm.tokenizer_manager.tokenizer
    query_ids = tokenizer.apply_chat_template(
        [{"role": "user", "content": sample["problem"]}],
        tokenize=True,
        add_generation_prompt=True
    )

    trace_response_ids = []
    iterations = 0
    prompt_ids = query_ids

    while iterations < max_iterations:
        # 第一轮：完整预算；后续：减去马尔可夫部分
        max_new = (context_size - markovian_size
                   if iterations > 0
                   else context_size)

        response = await llm.async_generate(
            input_ids=prompt_ids,
            sampling_params={"max_new_tokens": max_new},
            return_logprob=True
        )
        response_ids = _get_output_ids(response)
        trace_response_ids.append(response_ids)

        # 第一轮后保留前100个token
        if iterations == 0:
            query_ids = query_ids + response_ids[:100]

        # EOS检查
        if response["meta_info"]["finish_reason"]["type"] == "stop":
            break

        # 关键：下一轮prompt = query + 最后m个token
        prompt_ids = query_ids + response_ids[-markovian_size:]
        iterations += 1

    # 拼接所有生成的token
    return sample, sum(trace_response_ids, [])
```

### A.2 有效推理预算计算

```python
def calculate_effective_budget(C, m, I):
    """
    C: context_size (上下文窗口)
    m: markovian_size (马尔可夫状态大小)
    I: max_iterations (最大迭代次数)
    """
    return C + (I - 1) * (C - m)

# 示例
budget_24k = calculate_effective_budget(8192, 4096, 5)
print(f"24K配置: {budget_24k}")  # 输出: 24576

budget_96k = calculate_effective_budget(8192, 4096, 23)
print(f"96K配置: {budget_96k}")  # 输出: 98304
```

---

**作者注**：本文基于对 The Markovian Thinker 项目的深度分析，结合马尔可夫链理论和上下文工程实践撰写。代码片段来自 [McGill-NLP/the-markovian-thinker](https://github.com/McGill-NLP/the-markovian-thinker) 仓库。

**创作日期**：2025年10月27日

**更新记录**：
- 2025-10-27：初稿完成
